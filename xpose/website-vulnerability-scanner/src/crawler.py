import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

class Crawler:
    def __init__(self, base_url):
        self.base_url = base_url
        self.visited = set()
        self.urls_with_params = set()

    def crawl(self, url=None):
        if url is None:
            url = self.base_url

        if url in self.visited:
            return

        self.visited.add(url)
        parsed = urlparse(url)
        if parsed.query:
            self.urls_with_params.add(url)
            yield url

        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    full_url = urljoin(self.base_url, link['href'])
                    if self.base_url in full_url and full_url not in self.visited:
                        yield from self.crawl(full_url)
        except requests.RequestException:
            pass

    def get_visited_urls(self):
        return list(self.visited)

    def get_param_urls(self):
        return list(self.urls_with_params)