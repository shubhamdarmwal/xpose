import argparse
import re
import sys
import os
from datetime import datetime
from tqdm import tqdm

from crawler import Crawler
from scanner import Scanner
from report_generator import ReportGenerator

def print_banner():
    print(r"""
XXXXXXX       XXXXXXXPPPPPPPPPPPPPPPPP        OOOOOOOOO        SSSSSSSSSSSSSSS EEEEEEEEEEEEEEEEEEEEEE
X:::::X       X:::::XP::::::::::::::::P     OO:::::::::OO    SS:::::::::::::::SE::::::::::::::::::::E
X:::::X       X:::::XP::::::PPPPPP:::::P  OO:::::::::::::OO S:::::SSSSSS::::::SE::::::::::::::::::::E
X::::::X     X::::::XPP:::::P     P:::::PO:::::::OOO:::::::OS:::::S     SSSSSSSEE::::::EEEEEEEEE::::E
XXX:::::X   X:::::XXX  P::::P     P:::::PO::::::O   O::::::OS:::::S              E:::::E       EEEEEE
   X:::::X X:::::X     P::::P     P:::::PO:::::O     O:::::OS:::::S              E:::::E             
    X:::::X:::::X      P::::PPPPPP:::::P O:::::O     O:::::O S::::SSSS           E::::::EEEEEEEEEE   
     X:::::::::X       P:::::::::::::PP  O:::::O     O:::::O  SS::::::SSSSS      E:::::::::::::::E   
     X:::::::::X       P::::PPPPPPPPP    O:::::O     O:::::O    SSS::::::::SS    E:::::::::::::::E   
    X:::::X:::::X      P::::P            O:::::O     O:::::O       SSSSSS::::S   E::::::EEEEEEEEEE   
   X:::::X X:::::X     P::::P            O:::::O     O:::::O            S:::::S  E:::::E             
XXX:::::X   X:::::XXX  P::::P            O::::::O   O::::::O            S:::::S  E:::::E       EEEEEE
X::::::X     X::::::XPP::::::PP          O:::::::OOO:::::::OSSSSSSS     S:::::SEE::::::EEEEEEEE:::::E
X:::::X       X:::::XP::::::::P           OO:::::::::::::OO S::::::SSSSSS:::::SE::::::::::::::::::::E
X:::::X       X:::::XP::::::::P             OO:::::::::OO   S:::::::::::::::SS E::::::::::::::::::::E
XXXXXXX       XXXXXXXPPPPPPPPPP               OOOOOOOOO      SSSSSSSSSSSSSSS   EEEEEEEEEEEEEEEEEEEEEE
""")
    print("Welcome to XPOSE - Website Vulnerability Scanner\n")
    print("DISCLAIMER: Use this tool only on websites you own or have explicit permission to test.\n")

def is_valid_url(url):
    pattern = re.compile(
        r'^(http|https)://'
        r'([A-Za-z0-9.-]+)\.([A-Za-z]{2,})'
    )
    return re.match(pattern, url) is not None

def get_vuln_choice():
    print("Select vulnerability to scan for:")
    print("1. SQL Injection")
    print("2. Cross-Site Scripting (XSS)")
    print("3. Local File Inclusion (LFI)")
    print("4. Open Redirect")
    print("5. All")
    choice = input("Enter choice (1/2/3/4/5): ").strip()
    if choice == "1":
        return "sql"
    elif choice == "2":
        return "xss"
    elif choice == "3":
        return "lfi"
    elif choice == "4":
        return "redirect"
    else:
        print("Defaulting to all vulnerabilities.")
        return "all"

if __name__ == "__main__":
    print_banner()

    vuln_type = get_vuln_choice()
    parser = argparse.ArgumentParser(description="XPOSE - Website Vulnerability Scanner")
    parser.add_argument("url", nargs="?", help="Website URL to crawl (e.g., http://example.com)")
    parser.add_argument("--blacklist", nargs="*", default=[], help="List of URL substrings to blacklist")
    parser.add_argument("--whitelist", nargs="*", default=[], help="List of URL substrings to whitelist")
    parser.add_argument("--export-urls", action="store_true", help="Export all discovered URLs to urls.txt")
    parser.add_argument("--quiet", action="store_true", help="Quiet mode (minimal output)")
    args = parser.parse_args()

    base_url = args.url or input("Enter the website URL to crawl (e.g., http://example.com): ").strip()
    if not is_valid_url(base_url):
        print("Invalid URL format. Exiting.")
        sys.exit(1)

    try:
        # Step 1: Crawl the website with progress bar
        crawler = Crawler(base_url)
        print("Crawling website...")
        urls = []
        with tqdm(desc="Crawling", unit="url") as progress_bar:
            for url in crawler.crawl():
                tqdm.write(url)
                urls.append(url)
                progress_bar.update(1)

        param_urls = crawler.get_param_urls()

        if not param_urls:
            print("No parameterized URLs found during crawling. Exiting.")
            sys.exit(1)

        # Step 2: Scan the URLs for vulnerabilities
        print("Scanning URLs for vulnerabilities...")
        scanner = Scanner(
            param_urls,
            blacklist=args.blacklist,
            whitelist=args.whitelist,
            verbose=not args.quiet
        )
        scan_results = scanner.scan(vuln_type)

        # Save report in the current working directory
        domain = base_url.split("//")[-1].split("/")[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.abspath(f"{domain}_{timestamp}.html")
        print("Generating report...")
        report_generator = ReportGenerator(scan_results)
        report_generator.generate_report(output_file)

        # Export URLs if requested
        if args.export_urls:
            scanner.export_urls("urls.txt")

        # Print summary statistics
        scanner.print_summary()

        print("\nScan Summary:")
        print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total URLs scanned: {len(param_urls)}")
        print(f"Report saved to: {output_file}")

    except KeyboardInterrupt:
        print("\nScan interrupted by user. Generating partial report...")
        domain = base_url.split("//")[-1].split("/")[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.abspath(f"{domain}_{timestamp}_partial.html")
        report_generator = ReportGenerator(scan_results)
        report_generator.generate_report(output_file)
        print(f"Partial report saved to: {output_file}")

    except Exception as e:
        print(f"An error occurred: {e}")